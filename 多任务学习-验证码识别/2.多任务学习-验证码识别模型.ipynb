{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据集路径\n",
    "dataset_dir = \"./captcha/images/\"\n",
    "# 测试集占比\n",
    "num_test = 0.2\n",
    "# 批次大小\n",
    "batch_size = 32\n",
    "# 周期大小\n",
    "epochs = 100\n",
    "# 分类数\n",
    "num_classes = 10\n",
    "# 学习率\n",
    "lr = tf.Variable(0.001, dtype=tf.float32)\n",
    "# 是否是训练状态\n",
    "is_training = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取所有验证码图片路径和标签\n",
    "def get_filenames_and_classes(dataset_dir):\n",
    "    photo_filenames = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(dataset_dir):\n",
    "        # 获取文件路径\n",
    "        path = os.path.join(dataset_dir, filename)\n",
    "        photo_filenames.append(path)\n",
    "        label = filename[0:4]\n",
    "        num_labels = []\n",
    "        for i in range(4):\n",
    "            num_labels.append(int(label[i]))\n",
    "        labels.append(num_labels)\n",
    "    return photo_filenames,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取图片路径和标签\n",
    "photo_filenames,labels = get_filenames_and_classes(dataset_dir)\n",
    "photo_filenames = np.array(photo_filenames)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 打乱数据\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(photo_filenames)))\n",
    "photo_filenames_shuffled = photo_filenames[shuffle_indices]\n",
    "labels_shuffled = labels[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 切分训练集和测试集\n",
    "test_sample_index = -1 * int(num_test * float(len(photo_filenames)))\n",
    "x_train, x_test = photo_filenames_shuffled[:test_sample_index], photo_filenames_shuffled[test_sample_index:]\n",
    "y_train, y_test = labels_shuffled[:test_sample_index], labels_shuffled[test_sample_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 图像处理函数\n",
    "def parse_function(filenames, labels=None):\n",
    "    image = tf.read_file(filenames)\n",
    "    # 将图像解码\n",
    "    image = tf.image.decode_jpeg(image, channels=3)   \n",
    "    # resize图片大小\n",
    "    image = tf.image.resize_images(image, [224, 224])\n",
    "    # 图片预处理\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.subtract(image, 0.5)\n",
    "    image = tf.multiply(image, 2.0)\n",
    "\n",
    "    return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义两个placeholder\n",
    "features_placeholder = tf.placeholder(photo_filenames_shuffled.dtype, [None])\n",
    "labels_placeholder = tf.placeholder(labels_shuffled.dtype, [None, 4])\n",
    "\n",
    "# 创建dataset对象\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "# 处理图片\n",
    "dataset = dataset.map(parse_function)\n",
    "# 训练周期\n",
    "dataset = dataset.repeat(1)\n",
    "# 批次大小\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# 初始化迭代器\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "# 获得一个批次数据和标签\n",
    "data_batch, label_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alexnet(inputs, is_training=True):\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                         activation_fn=tf.nn.relu,\n",
    "                         weights_initializer=tf.glorot_uniform_initializer(),\n",
    "                         biases_initializer=tf.constant_initializer(0)):\n",
    "        \n",
    "        net = slim.conv2d(inputs, 64, [11, 11], 4)\n",
    "        net = slim.max_pool2d(net, [3, 3])\n",
    "        net = slim.conv2d(net, 192, [5, 5])\n",
    "        net = slim.max_pool2d(net, [3, 3])\n",
    "        net = slim.conv2d(net, 384, [3, 3])\n",
    "        net = slim.conv2d(net, 384, [3, 3])\n",
    "        net = slim.conv2d(net, 256, [3, 3])\n",
    "        net = slim.max_pool2d(net, [3, 3])\n",
    "        \n",
    "        # 数据扁平化\n",
    "        net = slim.flatten(net)\n",
    "        net = slim.fully_connected(net, 1024)\n",
    "        net = slim.dropout(net, is_training=is_training)\n",
    "        \n",
    "        net0 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "        net1 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "        net2 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "        net3 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "\n",
    "    return net0,net1,net2,net3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:loss=2.303 acc0=0.104 acc1=0.101 acc2=0.094 acc3=0.112 total_acc=0.001\n",
      "1:loss=2.303 acc0=0.111 acc1=0.101 acc2=0.099 acc3=0.102 total_acc=0.001\n",
      "2:loss=2.241 acc0=0.224 acc1=0.179 acc2=0.187 acc3=0.230 total_acc=0.001\n",
      "3:loss=2.181 acc0=0.317 acc1=0.226 acc2=0.245 acc3=0.306 total_acc=0.005\n",
      "4:loss=2.127 acc0=0.400 acc1=0.283 acc2=0.256 acc3=0.366 total_acc=0.014\n",
      "5:loss=2.050 acc0=0.494 acc1=0.356 acc2=0.319 acc3=0.455 total_acc=0.023\n",
      "6:loss=1.945 acc0=0.638 acc1=0.477 acc2=0.378 acc3=0.567 total_acc=0.065\n",
      "7:loss=1.906 acc0=0.699 acc1=0.518 acc2=0.410 acc3=0.591 total_acc=0.080\n",
      "8:loss=1.843 acc0=0.745 acc1=0.578 acc2=0.476 acc3=0.665 total_acc=0.137\n",
      "9:loss=1.815 acc0=0.733 acc1=0.613 acc2=0.519 acc3=0.712 total_acc=0.172\n",
      "10:loss=1.780 acc0=0.782 acc1=0.657 acc2=0.551 acc3=0.726 total_acc=0.223\n",
      "11:loss=1.723 acc0=0.804 acc1=0.688 acc2=0.688 acc3=0.768 total_acc=0.321\n",
      "12:loss=1.698 acc0=0.821 acc1=0.707 acc2=0.750 acc3=0.782 total_acc=0.378\n",
      "13:loss=1.680 acc0=0.832 acc1=0.731 acc2=0.775 acc3=0.785 total_acc=0.399\n",
      "14:loss=1.663 acc0=0.832 acc1=0.775 acc2=0.785 acc3=0.799 total_acc=0.436\n",
      "15:loss=1.654 acc0=0.823 acc1=0.812 acc2=0.799 acc3=0.799 total_acc=0.454\n",
      "16:loss=1.636 acc0=0.844 acc1=0.836 acc2=0.821 acc3=0.804 total_acc=0.499\n",
      "17:loss=1.633 acc0=0.833 acc1=0.843 acc2=0.832 acc3=0.804 total_acc=0.512\n",
      "18:loss=1.621 acc0=0.860 acc1=0.855 acc2=0.827 acc3=0.822 total_acc=0.530\n",
      "19:loss=1.627 acc0=0.849 acc1=0.854 acc2=0.828 acc3=0.811 total_acc=0.517\n",
      "20:loss=1.609 acc0=0.857 acc1=0.876 acc2=0.854 acc3=0.829 total_acc=0.561\n",
      "21:loss=1.607 acc0=0.853 acc1=0.885 acc2=0.860 acc3=0.821 total_acc=0.562\n",
      "22:loss=1.602 acc0=0.865 acc1=0.882 acc2=0.854 acc3=0.835 total_acc=0.571\n",
      "23:loss=1.600 acc0=0.859 acc1=0.888 acc2=0.856 acc3=0.839 total_acc=0.577\n",
      "24:loss=1.598 acc0=0.857 acc1=0.895 acc2=0.867 acc3=0.833 total_acc=0.573\n",
      "25:loss=1.604 acc0=0.851 acc1=0.894 acc2=0.853 acc3=0.828 total_acc=0.562\n",
      "26:loss=1.602 acc0=0.857 acc1=0.892 acc2=0.850 acc3=0.840 total_acc=0.582\n",
      "27:loss=1.592 acc0=0.864 acc1=0.902 acc2=0.878 acc3=0.829 total_acc=0.591\n",
      "28:loss=1.591 acc0=0.861 acc1=0.908 acc2=0.882 acc3=0.835 total_acc=0.598\n",
      "29:loss=1.587 acc0=0.870 acc1=0.912 acc2=0.872 acc3=0.843 total_acc=0.605\n",
      "30:loss=1.580 acc0=0.872 acc1=0.917 acc2=0.888 acc3=0.847 total_acc=0.624\n",
      "31:loss=1.576 acc0=0.875 acc1=0.919 acc2=0.897 acc3=0.848 total_acc=0.633\n",
      "32:loss=1.576 acc0=0.871 acc1=0.929 acc2=0.893 acc3=0.848 total_acc=0.630\n",
      "33:loss=1.576 acc0=0.868 acc1=0.918 acc2=0.901 acc3=0.847 total_acc=0.629\n",
      "34:loss=1.575 acc0=0.872 acc1=0.923 acc2=0.900 acc3=0.848 total_acc=0.630\n",
      "35:loss=1.573 acc0=0.875 acc1=0.932 acc2=0.899 acc3=0.850 total_acc=0.641\n",
      "36:loss=1.572 acc0=0.877 acc1=0.933 acc2=0.897 acc3=0.854 total_acc=0.644\n",
      "37:loss=1.573 acc0=0.871 acc1=0.926 acc2=0.904 acc3=0.849 total_acc=0.638\n",
      "38:loss=1.573 acc0=0.873 acc1=0.923 acc2=0.903 acc3=0.849 total_acc=0.641\n",
      "39:loss=1.574 acc0=0.868 acc1=0.928 acc2=0.904 acc3=0.853 total_acc=0.646\n",
      "40:loss=1.569 acc0=0.875 acc1=0.933 acc2=0.904 acc3=0.858 total_acc=0.656\n",
      "41:loss=1.572 acc0=0.878 acc1=0.927 acc2=0.905 acc3=0.846 total_acc=0.636\n",
      "42:loss=1.570 acc0=0.878 acc1=0.923 acc2=0.907 acc3=0.857 total_acc=0.651\n",
      "43:loss=1.568 acc0=0.879 acc1=0.936 acc2=0.905 acc3=0.857 total_acc=0.650\n",
      "44:loss=1.568 acc0=0.878 acc1=0.934 acc2=0.911 acc3=0.852 total_acc=0.658\n",
      "45:loss=1.569 acc0=0.873 acc1=0.929 acc2=0.896 acc3=0.865 total_acc=0.653\n",
      "46:loss=1.550 acc0=0.874 acc1=0.928 acc2=0.900 acc3=0.943 total_acc=0.715\n",
      "47:loss=1.546 acc0=0.873 acc1=0.928 acc2=0.902 acc3=0.953 total_acc=0.721\n",
      "48:loss=1.549 acc0=0.876 acc1=0.926 acc2=0.896 acc3=0.951 total_acc=0.710\n",
      "49:loss=1.549 acc0=0.876 acc1=0.922 acc2=0.897 acc3=0.955 total_acc=0.706\n",
      "50:loss=1.525 acc0=0.962 acc1=0.927 acc2=0.900 acc3=0.952 total_acc=0.791\n",
      "51:loss=1.523 acc0=0.961 acc1=0.930 acc2=0.907 acc3=0.953 total_acc=0.798\n",
      "52:loss=1.522 acc0=0.961 acc1=0.930 acc2=0.907 acc3=0.955 total_acc=0.800\n",
      "53:loss=1.523 acc0=0.962 acc1=0.929 acc2=0.900 acc3=0.953 total_acc=0.793\n",
      "54:loss=1.521 acc0=0.967 acc1=0.935 acc2=0.908 acc3=0.954 total_acc=0.810\n",
      "55:loss=1.524 acc0=0.965 acc1=0.927 acc2=0.900 acc3=0.953 total_acc=0.792\n",
      "56:loss=1.520 acc0=0.968 acc1=0.937 acc2=0.909 acc3=0.951 total_acc=0.807\n",
      "57:loss=1.518 acc0=0.969 acc1=0.939 acc2=0.911 acc3=0.955 total_acc=0.809\n",
      "58:loss=1.521 acc0=0.968 acc1=0.929 acc2=0.908 acc3=0.955 total_acc=0.808\n",
      "59:loss=1.520 acc0=0.965 acc1=0.936 acc2=0.908 acc3=0.953 total_acc=0.806\n",
      "60:loss=1.517 acc0=0.969 acc1=0.937 acc2=0.916 acc3=0.957 total_acc=0.820\n",
      "61:loss=1.517 acc0=0.969 acc1=0.944 acc2=0.911 acc3=0.956 total_acc=0.819\n",
      "62:loss=1.517 acc0=0.972 acc1=0.934 acc2=0.912 acc3=0.959 total_acc=0.818\n",
      "63:loss=1.515 acc0=0.973 acc1=0.941 acc2=0.914 acc3=0.961 total_acc=0.820\n",
      "64:loss=1.514 acc0=0.972 acc1=0.940 acc2=0.915 acc3=0.964 total_acc=0.825\n",
      "65:loss=1.518 acc0=0.968 acc1=0.939 acc2=0.908 acc3=0.958 total_acc=0.817\n",
      "66:loss=1.515 acc0=0.976 acc1=0.938 acc2=0.912 acc3=0.963 total_acc=0.824\n",
      "67:loss=1.514 acc0=0.969 acc1=0.943 acc2=0.913 acc3=0.962 total_acc=0.829\n",
      "68:loss=1.513 acc0=0.973 acc1=0.944 acc2=0.916 acc3=0.964 total_acc=0.829\n",
      "69:loss=1.515 acc0=0.969 acc1=0.939 acc2=0.915 acc3=0.962 total_acc=0.822\n",
      "70:loss=1.514 acc0=0.972 acc1=0.944 acc2=0.913 acc3=0.962 total_acc=0.828\n",
      "71:loss=1.514 acc0=0.972 acc1=0.949 acc2=0.913 acc3=0.962 total_acc=0.833\n",
      "72:loss=1.513 acc0=0.972 acc1=0.946 acc2=0.914 acc3=0.962 total_acc=0.830\n",
      "73:loss=1.514 acc0=0.969 acc1=0.943 acc2=0.920 acc3=0.960 total_acc=0.828\n",
      "74:loss=1.514 acc0=0.965 acc1=0.948 acc2=0.911 acc3=0.960 total_acc=0.827\n",
      "75:loss=1.513 acc0=0.972 acc1=0.942 acc2=0.912 acc3=0.963 total_acc=0.827\n",
      "76:loss=1.513 acc0=0.975 acc1=0.942 acc2=0.916 acc3=0.961 total_acc=0.827\n",
      "77:loss=1.514 acc0=0.974 acc1=0.940 acc2=0.914 acc3=0.963 total_acc=0.827\n",
      "78:loss=1.513 acc0=0.971 acc1=0.943 acc2=0.917 acc3=0.962 total_acc=0.827\n",
      "79:loss=1.513 acc0=0.975 acc1=0.941 acc2=0.915 acc3=0.963 total_acc=0.829\n",
      "80:loss=1.512 acc0=0.976 acc1=0.944 acc2=0.915 acc3=0.962 total_acc=0.828\n",
      "81:loss=1.513 acc0=0.973 acc1=0.942 acc2=0.915 acc3=0.965 total_acc=0.829\n",
      "82:loss=1.512 acc0=0.976 acc1=0.943 acc2=0.914 acc3=0.966 total_acc=0.832\n",
      "83:loss=1.512 acc0=0.971 acc1=0.945 acc2=0.915 acc3=0.964 total_acc=0.828\n",
      "84:loss=1.513 acc0=0.973 acc1=0.941 acc2=0.919 acc3=0.959 total_acc=0.829\n",
      "85:loss=1.512 acc0=0.976 acc1=0.944 acc2=0.916 acc3=0.962 total_acc=0.830\n",
      "86:loss=1.512 acc0=0.972 acc1=0.944 acc2=0.914 acc3=0.965 total_acc=0.828\n",
      "87:loss=1.511 acc0=0.974 acc1=0.947 acc2=0.915 acc3=0.963 total_acc=0.835\n",
      "88:loss=1.511 acc0=0.972 acc1=0.946 acc2=0.920 acc3=0.966 total_acc=0.836\n",
      "89:loss=1.512 acc0=0.973 acc1=0.941 acc2=0.920 acc3=0.962 total_acc=0.833\n",
      "90:loss=1.511 acc0=0.973 acc1=0.940 acc2=0.920 acc3=0.962 total_acc=0.831\n",
      "91:loss=1.511 acc0=0.975 acc1=0.943 acc2=0.924 acc3=0.966 total_acc=0.839\n",
      "92:loss=1.509 acc0=0.976 acc1=0.949 acc2=0.918 acc3=0.966 total_acc=0.841\n",
      "93:loss=1.509 acc0=0.975 acc1=0.948 acc2=0.922 acc3=0.965 total_acc=0.842\n",
      "94:loss=1.510 acc0=0.976 acc1=0.948 acc2=0.919 acc3=0.966 total_acc=0.838\n",
      "95:loss=1.509 acc0=0.976 acc1=0.948 acc2=0.919 acc3=0.965 total_acc=0.836\n",
      "96:loss=1.509 acc0=0.976 acc1=0.947 acc2=0.922 acc3=0.963 total_acc=0.839\n",
      "97:loss=1.509 acc0=0.976 acc1=0.948 acc2=0.918 acc3=0.965 total_acc=0.840\n",
      "98:loss=1.509 acc0=0.977 acc1=0.948 acc2=0.921 acc3=0.964 total_acc=0.843\n",
      "99:loss=1.508 acc0=0.978 acc1=0.949 acc2=0.923 acc3=0.965 total_acc=0.842\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 传入数据得到结果\n",
    "    logits0,logits1,logits2,logits3 = alexnet(data_batch, is_training)\n",
    "    # 定义loss\n",
    "    # sparse_softmax_cross_entropy：标签为整数\n",
    "    # softmax_cross_entropy：标签为one-hot独热编码\n",
    "    loss0 = tf.losses.sparse_softmax_cross_entropy(label_batch[:,0], logits0)\n",
    "    loss1 = tf.losses.sparse_softmax_cross_entropy(label_batch[:,1], logits1)\n",
    "    loss2 = tf.losses.sparse_softmax_cross_entropy(label_batch[:,2], logits2)\n",
    "    loss3 = tf.losses.sparse_softmax_cross_entropy(label_batch[:,3], logits3)\n",
    "    # 计算总的loss\n",
    "    total_loss = (loss0+loss1+loss2+loss3)/4.0\n",
    "    # 优化total_loss 优化器是：AdamOptimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(total_loss) \n",
    "    \n",
    "    # 计算准确率\n",
    "    correct0 = tf.nn.in_top_k(logits0, label_batch[:,0], 1) \n",
    "    accuracy0 = tf.reduce_mean(tf.cast(correct0, tf.float32))\n",
    "    correct1 = tf.nn.in_top_k(logits1, label_batch[:,1], 1) \n",
    "    accuracy1 = tf.reduce_mean(tf.cast(correct1, tf.float32))\n",
    "    correct2 = tf.nn.in_top_k(logits2, label_batch[:,2], 1) \n",
    "    accuracy2 = tf.reduce_mean(tf.cast(correct2, tf.float32))\n",
    "    correct3 = tf.nn.in_top_k(logits3, label_batch[:,3], 1) \n",
    "    accuracy3 = tf.reduce_mean(tf.cast(correct3, tf.float32))\n",
    "    # 总的准确率\n",
    "    total_correct = tf.cast(correct0, tf.float32)*tf.cast(correct1, tf.float32)*tf.cast(correct2, tf.float32)*tf.cast(correct3, tf.float32)\n",
    "    total_accuracy = tf.reduce_mean(tf.cast(total_correct, tf.float32))\n",
    "  \n",
    "    # 所有变量初始化  \n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    # 定义saver保存模型\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # 训练epochs个周期\n",
    "    for i in range(epochs):\n",
    "        if i%30 == 0:\n",
    "            sess.run(tf.assign(lr, lr/3))\n",
    "        # 训练集传入迭代器中\n",
    "        sess.run(iterator.initializer, feed_dict={features_placeholder: x_train,\n",
    "                                                  labels_placeholder: y_train})\n",
    "        # 训练模型\n",
    "        while True:\n",
    "            try:\n",
    "                sess.run(optimizer,feed_dict={is_training:True})\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                # 所有数据训练完毕后跳出循环\n",
    "                break\n",
    "        \n",
    "        # 测试集放入迭代器中\n",
    "        sess.run(iterator.initializer, feed_dict={features_placeholder: x_test,\n",
    "                                                  labels_placeholder: y_test})\n",
    "        # 测试结果\n",
    "        while True:\n",
    "            try:\n",
    "                # 获得准确率和loss值\n",
    "                acc0,acc1,acc2,acc3,total_acc,l = \\\n",
    "                    sess.run([accuracy0,accuracy1,accuracy2,accuracy3,total_accuracy,total_loss],feed_dict={is_training:False})\n",
    "                # loss值统计\n",
    "                tf.add_to_collection('sum_losses', l)\n",
    "                # 准确率统计\n",
    "                tf.add_to_collection('accuracy0', acc0)\n",
    "                tf.add_to_collection('accuracy1', acc1)\n",
    "                tf.add_to_collection('accuracy2', acc2)\n",
    "                tf.add_to_collection('accuracy3', acc3)\n",
    "                tf.add_to_collection('total_acc', total_acc)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                # loss值求平均\n",
    "                avg_loss = sess.run(tf.reduce_mean(tf.get_collection('sum_losses')))\n",
    "                # 准确率求平均\n",
    "                avg_acc0 = sess.run(tf.reduce_mean(tf.get_collection('accuracy0')))\n",
    "                avg_acc1 = sess.run(tf.reduce_mean(tf.get_collection('accuracy1')))\n",
    "                avg_acc2 = sess.run(tf.reduce_mean(tf.get_collection('accuracy2')))\n",
    "                avg_acc3 = sess.run(tf.reduce_mean(tf.get_collection('accuracy3')))\n",
    "                avg_total_acc = sess.run(tf.reduce_mean(tf.get_collection('total_acc')))\n",
    "                print('%d:loss=%.3f acc0=%.3f acc1=%.3f acc2=%.3f acc3=%.3f total_acc=%.3f' % \n",
    "                      (i,avg_loss,avg_acc0,avg_acc1,avg_acc2,avg_acc3,avg_total_acc))\n",
    "                # 清空loss统计\n",
    "                temp = tf.get_collection_ref('sum_losses')\n",
    "                del temp[:]\n",
    "                # 清空准确率统计\n",
    "                temp = tf.get_collection_ref('accuracy0')\n",
    "                del temp[:]\n",
    "                # 清空准确率统计\n",
    "                temp = tf.get_collection_ref('accuracy1')\n",
    "                del temp[:]\n",
    "                # 清空准确率统计\n",
    "                temp = tf.get_collection_ref('accuracy2')\n",
    "                del temp[:]\n",
    "                # 清空准确率统计\n",
    "                temp = tf.get_collection_ref('accuracy3')\n",
    "                del temp[:]\n",
    "                # 清空准确率统计\n",
    "                temp = tf.get_collection_ref('total_acc')\n",
    "                del temp[:]\n",
    "                # 所有数据测试完毕后跳出循环\n",
    "                break\n",
    "        \n",
    "    # 保存模型\n",
    "    saver.save(sess, 'models/model.ckpt', global_step = epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
